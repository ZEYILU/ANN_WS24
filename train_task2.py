import torch
import torch.optim as optim
import torch.nn as nn
import matplotlib.pyplot as plt
from model import CNN1
from data_loader import get_data_loaders

def plot_training_curves(train_losses, train_accs):
    """
    ç»˜åˆ¶è®­ç»ƒæŸå¤±å’Œå‡†ç¡®ç‡æ›²çº¿ã€‚

    å‚æ•°:
        train_losses (list): æ¯ä¸ª epoch çš„è®­ç»ƒæŸå¤±ã€‚
        train_accs (list): æ¯ä¸ª epoch çš„è®­ç»ƒå‡†ç¡®ç‡ã€‚
    """
    epochs = list(range(1, len(train_losses) + 1))
    plt.figure(figsize=(12, 5))

    # ç»˜åˆ¶ Loss æ›²çº¿
    plt.subplot(1, 2, 1)
    plt.plot(epochs, train_losses, label='Train Loss', marker='o')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training Loss per Epoch')
    plt.legend()
    plt.grid(True)

    # ç»˜åˆ¶ Accuracy æ›²çº¿
    plt.subplot(1, 2, 2)
    plt.plot(epochs, train_accs, label='Train Accuracy', marker='o')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.title('Training Accuracy per Epoch')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.show()

def train_task2(batch_size=64, num_epochs=20, learning_rate=0.001):
    """
    ä»»åŠ¡äºŒï¼šä½¿ç”¨ TRAIN + TEST ä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œè®­ç»ƒ CNN1 æ¨¡å‹ã€‚

    å‚æ•°:
        batch_size (int): è®­ç»ƒæ—¶çš„ batch sizeã€‚
        num_epochs (int): è®­ç»ƒè½®æ•°ã€‚
        learning_rate (float): å­¦ä¹ ç‡ã€‚

    è¿”å›:
        model (nn.Module): è®­ç»ƒåçš„ PyTorch æ¨¡å‹ã€‚
        train_losses (list): æ¯ä¸ª epoch çš„è®­ç»ƒæŸå¤±ã€‚
        train_accs (list): æ¯ä¸ª epoch çš„è®­ç»ƒå‡†ç¡®ç‡ã€‚
    """
    print("ğŸš€ ä»»åŠ¡äºŒï¼šä½¿ç”¨ TRAIN + TEST è®­ç»ƒæ¨¡å‹...")
    
    # ä»»åŠ¡äºŒçš„å…³é”®ï¼šè®© `TEST` æ•°æ®ä¹Ÿä½œä¸ºè®­ç»ƒæ•°æ®
    train_loader, test_loader, test2_loader = get_data_loaders(batch_size=batch_size, preprocess_test=True)

    # **æ‹¼æ¥ `TRAIN` å’Œ `TEST` æ•°æ®é›†**
    full_train_loader = torch.utils.data.DataLoader(
        train_loader.dataset + test_loader.dataset, batch_size=batch_size, shuffle=True
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    num_classes = len(train_loader.dataset.classes)
    model = CNN1(num_classes=num_classes).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    def train_one_epoch(model, dataloader, criterion, optimizer):
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * inputs.size(0)
            _, predicted = torch.max(outputs, 1)
            correct += (predicted == labels).sum().item()
            total += labels.size(0)

        epoch_loss = running_loss / total
        epoch_acc = correct / total
        return epoch_loss, epoch_acc

    train_losses = []
    train_accs = []

    for epoch in range(num_epochs):
        train_loss, train_acc = train_one_epoch(model, full_train_loader, criterion, optimizer)
        train_losses.append(train_loss)
        train_accs.append(train_acc)

        print(f"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")

    model_filename = "task2_model.pth"
    torch.save(model.state_dict(), model_filename)
    print(f"âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜ä¸º {model_filename}")

    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_training_curves(train_losses, train_accs)

    return model, train_losses, train_accs
